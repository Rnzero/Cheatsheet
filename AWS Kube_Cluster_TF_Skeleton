We need:

1) VPC
2) EKS Control Plane
3) Worker capacity management
4) Terraform state file in S3


-> Tools: 
AWS Services: IAM, VPC, EKS, EC2, ENI, S3, KMS keys
Installations: Terraform, AWS Cli, kubectl, eksctl


-> Terraform directory layout: Refer to https://registry.terraform.io/browse/modules?provider=aws for all AWS modules
    --Environment
      --Non_Prod
        -main.tf
        -variables.tf
        -terraform.tfvars
        -backend.tf

-> Create State file on S3 -> State locking


-> Create VPC and EKS modules on main terraform file: 


-> provisioning the cluster:
terraform init
terraform plan -out tfplan -var-file="terraform.tfvars"
terraform apply tfplan

-> configure kubectl:
aws eks update-kubeconfig --name my-cluster --region us-east-1
kubectl get nodes


-> IAM roles for Clusters (IRSA) : https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html?utm_source=chatgpt.com


-> Pod security checklist:
Use IRSA for pod permissions (no node IAM secrets). 
Lock Terraform state and enable S3 versioning. 
Enable EKS control plane logging and audit logs.
Use network policies, security groups for pods, and private subnets for nodes.
Use kube2iam/kiam alternatives are deprecated; prefer IRSA.
Regularly scan Terraform with tflint, tfsec, checkov and enforce terraform fmt/validate in CI.



